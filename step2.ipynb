{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import config\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "from pyspark.sql.functions import col, lit, explode\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType, TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/csuftitan/Documents/Development/Adv%20DB%20Project/marketDataAnalysis/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/csuftitan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/csuftitan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2e42ebb2-e7e2-4962-b4c7-069373c666bf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 342ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2e42ebb2-e7e2-4962-b4c7-069373c666bf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/11ms)\n",
      "23/11/29 03:11:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session with the Kafka package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MarketDataProcessor\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kafka configuration for Spark\n",
    "# kafka_params = {\n",
    "#     \"kafka.bootstrap.servers\": \"localhost:9092\",\n",
    "#     \"subscribe\": \"marketdata\"\n",
    "# }\n",
    "\n",
    "# # Define the schema for the incoming JSON data\n",
    "# # Adjusted schema definition to match the JSON structure\n",
    "# schema = StructType([\n",
    "#     StructField(\"Meta Data\", StructType([\n",
    "#         StructField(\"1. Information\", StringType(), True),\n",
    "#         StructField(\"2. Symbol\", StringType(), True),\n",
    "#         StructField(\"3. Last Refreshed\", TimestampType(), True),\n",
    "#         StructField(\"4. Interval\", StringType(), True),\n",
    "#         StructField(\"5. Output Size\", StringType(), True),\n",
    "#         StructField(\"6. Time Zone\", StringType(), True),\n",
    "#     ]), True),\n",
    "#     StructField(\"Time Series (5min)\", MapType(\n",
    "#         StringType(),\n",
    "#         StructType([\n",
    "#             StructField(\"1. open\", StringType(), True),\n",
    "#             StructField(\"2. high\", StringType(), True),\n",
    "#             StructField(\"3. low\", StringType(), True),\n",
    "#             StructField(\"4. close\", StringType(), True),\n",
    "#             StructField(\"5. volume\", StringType(), True),\n",
    "#         ])\n",
    "#     ), True),\n",
    "# ])\n",
    "\n",
    "# # Read data from Kafka\n",
    "# df = spark.readStream.format(\"kafka\").options(**kafka_params).load().select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "\n",
    "# # # Create a PySpark DataFrame\n",
    "# # df = spark.createDataFrame([(df[\"\"])], schema=schema)\n",
    "# # print(\"=-=======================\")\n",
    "# # df.head()\n",
    "\n",
    "# # df = df.select(\"parsed_value.Meta Data.`2. Symbol`\",\"parsed_value.Time Series (5min).`1. open`\") #, 'parsed_value.Time Series (5min).2. high','parsed_value.Time Series (5min).3. low','parsed_value.Time Series (5min).4. close', 'parsed_value.Time Series (5min).5. value')\n",
    "\n",
    "# # df = spark.createDataFrame([(df[\"parsed_value.Meta Data\"][\"`2. Symbol`\"])], schema=schema)\n",
    "\n",
    "# # df.show()\n",
    "# # Start running the query that prints the running counts to the console\n",
    "# # query = df\\\n",
    "# #     .writeStream\\\n",
    "# #     .outputMode('append')\\\n",
    "# #     .option(\"truncate\", \"false\")\\\n",
    "# #     .format('console')\\\n",
    "# #     .start()\\\n",
    "# #     .awaitTermination()\n",
    "\n",
    "# query = df \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"memory\") \\\n",
    "#     .queryName(\"my_table\") \\\n",
    "#     .start()\n",
    "# # Wait for the streaming query to start\n",
    "# query.awaitTermination()\n",
    "\n",
    "# # Query the in-memory table using Spark SQL to create a static DataFrame\n",
    "# static_df = spark.sql(\"SELECT * FROM my_table\")\n",
    "\n",
    "# # Show the resulting static DataFrame\n",
    "# static_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration for Spark\n",
    "# kafka_params = {\n",
    "#     \"kafka.bootstrap.servers\": \"localhost:9092\",\n",
    "#     \"subscribe\": \"marketdata\"\n",
    "# }\n",
    "\n",
    "\n",
    "# schema = (\n",
    "#         StructType()\n",
    "#         .add(\"symbol\", StringType())\n",
    "#         .add(\"timeseries\", StringType())\n",
    "#         .add(\"close\", StringType())\n",
    "#     )\n",
    "\n",
    "# df = spark\\\n",
    "#       .readStream \\\n",
    "#       .format(\"kafka\") \\\n",
    "#       .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#       .option(\"subscribe\", \"marketdata\") \\\n",
    "#       .load()\n",
    "      \n",
    "\n",
    "# query = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"truncate\", \"false\")\\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "# Define the schema for the incoming JSON data\n",
    "# Adjusted schema definition to match the JSON structure\n",
    "\n",
    "\n",
    "# Read data from Kafka into a DataFrame\n",
    "# kafka_stream_df = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .options(**kafka_params) \\\n",
    "#     .load() \\\n",
    "#     .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "#    .select(from_json(\"value\", schema).alias(\"json_data\")) \\\n",
    "#     .select(\"json_data.*\")\n",
    "\n",
    "# kafka_stream_df = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .options(**kafka_params) \\\n",
    "#     .load() \\\n",
    "#     .select([\"*\"])\n",
    "\n",
    "# Perform any additional transformations as needed\n",
    "# kafka_stream_df.printSchema()\n",
    "# # Output the DataFrame to the console for testing\n",
    "# query = kafka_stream_df \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "\n",
    "# writing_df = kafka_stream_df.writeStream \\\n",
    "#     .format(\"json\") \\\n",
    "#     .option(\"path\", \"data/output/device_data\") \\\n",
    "#     .option(\"checkpointLocation\",\"checkpoint_dir\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .start()\n",
    "\n",
    "# Wait for the streaming query to finish\n",
    "# query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 03:17:50 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/3h/dgk1gddd4vd7jthqz32mnkhh0000gn/T/temporary-1d2d1d23-4800-4ae6-8362-caaca56eafb7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/29 03:17:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/29 03:17:50 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/11/29 03:17:50 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/11/29 03:17:50 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/11/29 03:17:50 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/11/29 03:17:50 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- timeseries: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      "\n",
      "+------+---------------------+----------+\n",
      "|symbol|timeseries           |close     |\n",
      "+------+---------------------+----------+\n",
      "|AAPL  |\"2023-11-28 19:55:00\"|\"189.9400\"|\n",
      "|AAPL  |\"2023-11-28 19:50:00\"|\"189.8350\"|\n",
      "|AAPL  |\"2023-11-28 19:45:00\"|\"189.6600\"|\n",
      "|AAPL  |\"2023-11-28 19:40:00\"|\"189.6500\"|\n",
      "|AAPL  |\"2023-11-28 19:35:00\"|\"189.6500\"|\n",
      "|AAPL  |\"2023-11-28 19:30:00\"|\"189.6550\"|\n",
      "|AAPL  |\"2023-11-28 19:25:00\"|\"189.6400\"|\n",
      "|AAPL  |\"2023-11-28 19:20:00\"|\"189.7400\"|\n",
      "|AAPL  |\"2023-11-28 19:15:00\"|\"189.6400\"|\n",
      "|AAPL  |\"2023-11-28 19:10:00\"|\"189.6400\"|\n",
      "|AAPL  |\"2023-11-28 19:05:00\"|\"189.6900\"|\n",
      "|AAPL  |\"2023-11-28 19:00:00\"|\"189.7700\"|\n",
      "|AAPL  |\"2023-11-28 18:55:00\"|\"189.7420\"|\n",
      "|AAPL  |\"2023-11-28 18:50:00\"|\"189.7700\"|\n",
      "|AAPL  |\"2023-11-28 18:45:00\"|\"189.7800\"|\n",
      "|AAPL  |\"2023-11-28 18:40:00\"|\"189.7900\"|\n",
      "|AAPL  |\"2023-11-28 18:35:00\"|\"189.7500\"|\n",
      "|AAPL  |\"2023-11-28 18:30:00\"|\"189.6500\"|\n",
      "|AAPL  |\"2023-11-28 18:25:00\"|\"189.6000\"|\n",
      "|AAPL  |\"2023-11-28 18:20:00\"|\"189.4950\"|\n",
      "+------+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:9092\",\n",
    "    \"subscribe\": \"marketdata\"\n",
    "}\n",
    "\n",
    "\n",
    "# Define the schema for the \"value\" field\n",
    "value_schema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"timeseries\", StringType(), True),\n",
    "    StructField(\"close\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Read data from Kafka topic\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"marketdata\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "kafka_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), value_schema) \\\n",
    "                        .alias(\"values\"))\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "parsed_df = kafka_df.select(\"values.*\")\n",
    "\n",
    "# qry = parsed_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# qry.awaitTermination()\n",
    "\n",
    "# Write the streaming DataFrame to an in-memory table\n",
    "query = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"my_table3\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to start\n",
    "query.awaitTermination(10)\n",
    "\n",
    "# Query the in-memory table using Spark SQL to create a static DataFrame\n",
    "static_df = spark.sql(\"SELECT * FROM my_table3\")\n",
    "\n",
    "# Show the resulting static DataFrame\n",
    "# static_df.show(truncate=False)\n",
    "static_df.printSchema()\n",
    "\n",
    "from pyspark.sql.functions import split, expr\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df = static_df.withColumn(\"time\", expr(\"split(trim('[]', timeseries), ',' )\"))\n",
    "df = df.withColumn(\"value\", expr(\"split(trim('[]', close), ',' )\"))\n",
    "\n",
    "df = df.withColumn(\"new\", F.arrays_zip(\"time\", \"value\"))\\\n",
    "       .withColumn(\"new\", F.explode(\"new\"))\\\n",
    "       .select(\"symbol\", F.col(\"new.time\").alias(\"timeseries\"), F.col(\"new.value\").alias(\"close\"))\n",
    "\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
